{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\denis\\anaconda3\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = scipy.io.loadmat('cupDataset.mat')\n",
    "# mat.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['           cupImagename                   cup       \\n',\n",
       " '    ___________________________    _________________\\n',\n",
       " '\\n',\n",
       " \"    {'cup_images\\\\cup(1).jpg'  }    {[147 57 67 105]}\\n\",\n",
       " \"    {'cup_images\\\\cup(10).jpg' }    {[  18 61 48 99]}\\n\",\n",
       " \"    {'cup_images\\\\cup(100).jpg'}    {[ 156 69 58 95]}\\n\",\n",
       " \"    {'cup_images\\\\cup(101).jpg'}    {[ 171 9 53 165]}\\n\",\n",
       " \"    {'cup_images\\\\cup(102).jpg'}    {[  38 88 21 67]}\\n\",\n",
       " \"    {'cup_images\\\\cup(103).jpg'}    {[148 28 72 187]}\\n\",\n",
       " \"    {'cup_images\\\\cup(104).jpg'}    {[  2 51 49 125]}\\n\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data_lines = file.readlines()\n",
    "\n",
    "# Display the first few lines to understand the structure of the data\n",
    "data_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to parse a single line of data\n",
    "def parse_line(line):\n",
    "    # Using regular expression to extract the image name and the corresponding data\n",
    "    match = re.match(r\"\\s*{'(.*?)'}\\s*{\\[(.*?)\\]}\", line)\n",
    "    if match:\n",
    "        image_name = match.group(1)\n",
    "        cup_data = list(map(int, match.group(2).split()))\n",
    "        return {'cupImagename': image_name, 'cup': cup_data}\n",
    "    return None\n",
    "\n",
    "# Parse each line and create a list of dictionaries\n",
    "parsed_data = [parse_line(line) for line in data_lines if line.strip() and not line.startswith('cupImagename')]\n",
    "\n",
    "# Remove None values (lines that did not match the pattern)\n",
    "parsed_data = [data for data in parsed_data if data is not None]\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "df = pd.DataFrame(parsed_data)\n",
    "df['cup'] = df['cup'].apply(lambda x: [float(i) for i in x])\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few rows of the cleaned DataFrame\n",
    "df.head()\n",
    "df.to_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class CupDatasetObjectDetection(Dataset):\n",
    "    def validate_and_correct_box(self, box):\n",
    "        \"\"\"\n",
    "        Ensure that the bounding box has positive width and height.\n",
    "        If not, correct the box or exclude it.\n",
    "        \"\"\"\n",
    "        xmin, ymin, xmax, ymax = box\n",
    "        if xmax <= xmin or ymax <= ymin:\n",
    "            # Correct the box or return None to exclude it\n",
    "            return [min(xmin, xmax), min(ymin, ymax), max(xmin, xmax), max(ymin, ymax)]\n",
    "        return box\n",
    "\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "        self.image_paths = []\n",
    "        self.bounding_boxes = []\n",
    "\n",
    "        for index, row in dataframe.iterrows():\n",
    "            img_name = row['cupImagename']\n",
    "            if root_dir in img_name:\n",
    "                full_path = img_name\n",
    "            else:\n",
    "                full_path = os.path.join(self.root_dir, img_name)\n",
    "            if os.path.exists(full_path) and full_path.endswith(('.jpg', '.png', '.jpeg')):\n",
    "                self.image_paths.append(full_path)\n",
    "                self.bounding_boxes.append(row['cup'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        box = self.bounding_boxes[idx]\n",
    "        box = self.validate_and_correct_box(box)\n",
    "        if box is None:\n",
    "            # Handle the case of an invalid box, e.g., skip this item\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "            \n",
    "        box_tensor = torch.as_tensor(box, dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64) \n",
    " \n",
    "        target = {\n",
    "            \"boxes\": box_tensor.unsqueeze(0), \n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": (box_tensor[3] - box_tensor[1]) * (box_tensor[2] - box_tensor[0]),\n",
    "            \"iscrowd\": torch.zeros((1,), dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "        return image, target\n",
    "\n",
    "# Define your desired size and transformations\n",
    "desired_size = (224, 224)  # Example size, change according to your needs\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(desired_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Assuming 'df' is your DataFrame and 'cup_images' is the directory containing images\n",
    "dataset = CupDatasetObjectDetection(df, root_dir='cup_images', transform=transform)\n",
    "\n",
    "# Create DataLoader\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for handling batches of images and targets.\n",
    "    \"\"\"\n",
    "    images = [item[0] for item in batch]\n",
    "    targets = [item[1] for item in batch]\n",
    "\n",
    "    # No need to collate images as they are already tensors\n",
    "    # But ensure targets are in a list\n",
    "    return images, targets\n",
    "\n",
    "# Use the custom collate function in DataLoader\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\denis\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\denis\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 69.,  49., 138., 113.]]), 'labels': tensor([1]), 'image_id': tensor([104]), 'area': tensor(4416.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 56.,  54., 165., 102.]]), 'labels': tensor([1]), 'image_id': tensor([46]), 'area': tensor(5232.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 42.,  62., 155.,  86.]]), 'labels': tensor([1]), 'image_id': tensor([156]), 'area': tensor(2712.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 45.,  70., 162., 109.]]), 'labels': tensor([1]), 'image_id': tensor([17]), 'area': tensor(4563.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 71.,  46., 146., 133.]]), 'labels': tensor([1]), 'image_id': tensor([97]), 'area': tensor(6525.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 33.,  58.,  58., 117.]]), 'labels': tensor([1]), 'image_id': tensor([66]), 'area': tensor(1475.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 47.,  36., 118., 105.]]), 'labels': tensor([1]), 'image_id': tensor([175]), 'area': tensor(4899.), 'iscrowd': tensor([0])}, {'boxes': tensor([[  6.,  37.,  68., 131.]]), 'labels': tensor([1]), 'image_id': tensor([65]), 'area': tensor(5828.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 56.,  86., 167.,  89.]]), 'labels': tensor([1]), 'image_id': tensor([36]), 'area': tensor(333.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 28.,  60., 150.,  91.]]), 'labels': tensor([1]), 'image_id': tensor([14]), 'area': tensor(3782.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 40.,  74., 163.,  83.]]), 'labels': tensor([1]), 'image_id': tensor([124]), 'area': tensor(1107.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 47.,  63., 127.,  87.]]), 'labels': tensor([1]), 'image_id': tensor([155]), 'area': tensor(1920.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 1., 71., 57., 83.]]), 'labels': tensor([1]), 'image_id': tensor([20]), 'area': tensor(672.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 70.,  51., 152., 151.]]), 'labels': tensor([1]), 'image_id': tensor([7]), 'area': tensor(8200.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 38.,  51., 184., 100.]]), 'labels': tensor([1]), 'image_id': tensor([161]), 'area': tensor(7154.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 54.,  44., 123., 141.]]), 'labels': tensor([1]), 'image_id': tensor([163]), 'area': tensor(6693.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 45.,  46., 155.,  89.]]), 'labels': tensor([1]), 'image_id': tensor([179]), 'area': tensor(4730.), 'iscrowd': tensor([0])}, {'boxes': tensor([[21., 67., 38., 88.]]), 'labels': tensor([1]), 'image_id': tensor([2]), 'area': tensor(357.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 45.,  27., 155., 131.]]), 'labels': tensor([1]), 'image_id': tensor([138]), 'area': tensor(11440.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 64.,  47., 150., 118.]]), 'labels': tensor([1]), 'image_id': tensor([48]), 'area': tensor(6106.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[  1.,   3.,  67., 110.]]), 'labels': tensor([1]), 'image_id': tensor([21]), 'area': tensor(7062.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 49.,  57., 173.,  61.]]), 'labels': tensor([1]), 'image_id': tensor([99]), 'area': tensor(496.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 86.,  23., 128., 117.]]), 'labels': tensor([1]), 'image_id': tensor([28]), 'area': tensor(3948.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 28.,  82.,  62., 142.]]), 'labels': tensor([1]), 'image_id': tensor([38]), 'area': tensor(2040.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 55.,  63., 148., 119.]]), 'labels': tensor([1]), 'image_id': tensor([176]), 'area': tensor(5208.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 46.,  60., 176., 114.]]), 'labels': tensor([1]), 'image_id': tensor([154]), 'area': tensor(7020.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 1., 73., 49., 99.]]), 'labels': tensor([1]), 'image_id': tensor([8]), 'area': tensor(1248.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 33.,  57.,  81., 135.]]), 'labels': tensor([1]), 'image_id': tensor([152]), 'area': tensor(3744.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 49.,  80., 175.,  89.]]), 'labels': tensor([1]), 'image_id': tensor([39]), 'area': tensor(1134.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 18.,  55.,  51., 128.]]), 'labels': tensor([1]), 'image_id': tensor([134]), 'area': tensor(2409.), 'iscrowd': tensor([0])}, {'boxes': tensor([[23., 63., 49., 87.]]), 'labels': tensor([1]), 'image_id': tensor([5]), 'area': tensor(624.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 48.,  97.,  58., 111.]]), 'labels': tensor([1]), 'image_id': tensor([67]), 'area': tensor(140.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 38.,  77., 160.,  94.]]), 'labels': tensor([1]), 'image_id': tensor([130]), 'area': tensor(2074.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 63.,  36., 159., 106.]]), 'labels': tensor([1]), 'image_id': tensor([103]), 'area': tensor(6720.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 14.,  54.,  56., 105.]]), 'labels': tensor([1]), 'image_id': tensor([50]), 'area': tensor(2142.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 43.,  65., 151.,  76.]]), 'labels': tensor([1]), 'image_id': tensor([140]), 'area': tensor(1188.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 55.,  86., 169., 111.]]), 'labels': tensor([1]), 'image_id': tensor([16]), 'area': tensor(2850.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 35.,  38.,  92., 108.]]), 'labels': tensor([1]), 'image_id': tensor([170]), 'area': tensor(3990.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 65.,  54., 147.,  96.]]), 'labels': tensor([1]), 'image_id': tensor([92]), 'area': tensor(3444.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 40.,  54., 153., 149.]]), 'labels': tensor([1]), 'image_id': tensor([168]), 'area': tensor(10735.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 54.,  47., 160.,  78.]]), 'labels': tensor([1]), 'image_id': tensor([141]), 'area': tensor(3286.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 61.,  40., 161., 117.]]), 'labels': tensor([1]), 'image_id': tensor([43]), 'area': tensor(7700.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 60.,  76., 164., 103.]]), 'labels': tensor([1]), 'image_id': tensor([25]), 'area': tensor(2808.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 63.,  52., 146., 104.]]), 'labels': tensor([1]), 'image_id': tensor([142]), 'area': tensor(4316.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[  6.,  66.,  82., 146.]]), 'labels': tensor([1]), 'image_id': tensor([45]), 'area': tensor(6080.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 38.,  51., 159.,  54.]]), 'labels': tensor([1]), 'image_id': tensor([96]), 'area': tensor(363.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 46.,  52., 150.,  76.]]), 'labels': tensor([1]), 'image_id': tensor([91]), 'area': tensor(2496.), 'iscrowd': tensor([0])}, {'boxes': tensor([[  8.,  21.,  71., 174.]]), 'labels': tensor([1]), 'image_id': tensor([70]), 'area': tensor(9639.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[24., 74., 62., 93.]]), 'labels': tensor([1]), 'image_id': tensor([52]), 'area': tensor(722.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 7., 61., 57., 89.]]), 'labels': tensor([1]), 'image_id': tensor([19]), 'area': tensor(1400.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 59.,  63., 151., 101.]]), 'labels': tensor([1]), 'image_id': tensor([112]), 'area': tensor(3496.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 65.,  74., 152., 111.]]), 'labels': tensor([1]), 'image_id': tensor([69]), 'area': tensor(3219.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 59.,  57., 165.,  89.]]), 'labels': tensor([1]), 'image_id': tensor([26]), 'area': tensor(3392.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 27.,  30., 140., 137.]]), 'labels': tensor([1]), 'image_id': tensor([180]), 'area': tensor(12091.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 38.,  59., 178.,  61.]]), 'labels': tensor([1]), 'image_id': tensor([51]), 'area': tensor(280.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 30.,  50.,  36., 124.]]), 'labels': tensor([1]), 'image_id': tensor([160]), 'area': tensor(444.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 64.,  77., 135.,  93.]]), 'labels': tensor([1]), 'image_id': tensor([100]), 'area': tensor(1136.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 66.,  46., 158., 122.]]), 'labels': tensor([1]), 'image_id': tensor([111]), 'area': tensor(6992.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 47.,  56., 144.,  81.]]), 'labels': tensor([1]), 'image_id': tensor([102]), 'area': tensor(2425.), 'iscrowd': tensor([0])}, {'boxes': tensor([[  1.,  92.,  56., 123.]]), 'labels': tensor([1]), 'image_id': tensor([11]), 'area': tensor(1705.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 57.,  58., 165.,  93.]]), 'labels': tensor([1]), 'image_id': tensor([12]), 'area': tensor(3780.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 50.,  88., 174.,  93.]]), 'labels': tensor([1]), 'image_id': tensor([40]), 'area': tensor(620.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 33.,  62., 181.,  70.]]), 'labels': tensor([1]), 'image_id': tensor([149]), 'area': tensor(1184.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 42.,  64., 162.,  67.]]), 'labels': tensor([1]), 'image_id': tensor([85]), 'area': tensor(360.), 'iscrowd': tensor([0])}]\n",
      "Images type: <class 'list'>\n",
      "Targets type: [{'boxes': tensor([[ 47.,  64., 160., 115.]]), 'labels': tensor([1]), 'image_id': tensor([117]), 'area': tensor(5763.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 47.,  45., 151., 117.]]), 'labels': tensor([1]), 'image_id': tensor([181]), 'area': tensor(7488.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 48.,  61., 157.,  86.]]), 'labels': tensor([1]), 'image_id': tensor([59]), 'area': tensor(2725.), 'iscrowd': tensor([0])}, {'boxes': tensor([[ 44.,  62., 173.,  62.]]), 'labels': tensor([1]), 'image_id': tensor([113]), 'area': tensor(0.), 'iscrowd': tensor([0])}]\n",
      "Target 0: tensor([[ 47.,  64., 160., 115.]], device='cuda:0')\n",
      "Target 1: tensor([[ 47.,  45., 151., 117.]], device='cuda:0')\n",
      "Target 2: tensor([[ 48.,  61., 157.,  86.]], device='cuda:0')\n",
      "Target 3: tensor([[ 44.,  62., 173.,  62.]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "All bounding boxes should have positive height and width. Found invalid box [157.14285278320312, 221.42857360839844, 617.8571166992188, 221.42857360839844] for target at index 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14748\\2630297387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Target {i}: {t['boxes']}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14748\\2630297387.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[1;31m# ... rest of the training loop ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mAssertionError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     93\u001b[0m                     \u001b[0mbb_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdegenerate_boxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                     \u001b[0mdegen_bb\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbb_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                     torch._assert(\n\u001b[0m\u001b[0;32m     96\u001b[0m                         \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                         \u001b[1;34m\"All bounding boxes should have positive height and width.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   1207\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1209\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: All bounding boxes should have positive height and width. Found invalid box [157.14285278320312, 221.42857360839844, 617.8571166992188, 221.42857360839844] for target at index 3."
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# Load a pre-trained model for fine-tuning\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "num_classes = 2  # 1 class (cup) + background\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the head of the classifier with a new one (for our number of classes)\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# Move model to the right device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10  # You can adjust this\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, targets in data_loader:\n",
    "        print(\"Images type:\", type(images))\n",
    "        print(\"Targets type:\", targets)\n",
    "        if isinstance(targets, list) and all(isinstance(t, dict) for t in targets):\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "\n",
    "            try:\n",
    "                loss_dict = model(images, targets)\n",
    "                # ... rest of the training loop ...\n",
    "            except AssertionError as e:\n",
    "                for i, t in enumerate(targets):\n",
    "                    print(f\"Target {i}: {t['boxes']}\")\n",
    "                raise e\n",
    "\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "        else:\n",
    "            print(\"Error: targets are not in the correct format\")\n",
    "\n",
    "    print(f\"Epoch {epoch} - Loss: {losses.item()}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'cup_detection_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
