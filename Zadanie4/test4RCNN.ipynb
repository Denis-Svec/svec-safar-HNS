{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "from torchvision.io import read_image\n",
    "from torchvision.ops.boxes import masks_to_boxes\n",
    "from torchvision import tv_tensors\n",
    "from torchvision.transforms.v2 import functional as F\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0+cu118\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat = scipy.io.loadmat('cupDataset.mat')\n",
    "# mat.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['           cupImagename                   cup       \\n',\n",
       " '    ___________________________    _________________\\n',\n",
       " '\\n',\n",
       " \"    {'cup_images\\\\cup(1).jpg'  }    {[147 57 67 105]}\\n\",\n",
       " \"    {'cup_images\\\\cup(10).jpg' }    {[  18 61 48 99]}\\n\",\n",
       " \"    {'cup_images\\\\cup(100).jpg'}    {[ 156 69 58 95]}\\n\",\n",
       " \"    {'cup_images\\\\cup(101).jpg'}    {[ 171 9 53 165]}\\n\",\n",
       " \"    {'cup_images\\\\cup(102).jpg'}    {[  38 88 21 67]}\\n\",\n",
       " \"    {'cup_images\\\\cup(103).jpg'}    {[148 28 72 187]}\\n\",\n",
       " \"    {'cup_images\\\\cup(104).jpg'}    {[  2 51 49 125]}\\n\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = 'data.txt'\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data_lines = file.readlines()\n",
    "\n",
    "data_lines[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def parse_line(line):\n",
    " \n",
    "    match = re.match(r\"\\s*{'(.*?)'}\\s*{\\[(.*?)\\]}\", line)\n",
    "    if match:\n",
    "        image_name = match.group(1)\n",
    "        cup_data = list(map(int, match.group(2).split()))\n",
    "        return {'cupImagename': image_name, 'cup': cup_data}\n",
    "    return None\n",
    "\n",
    "parsed_data = [parse_line(line) for line in data_lines if line.strip() and not line.startswith('cupImagename')]\n",
    "\n",
    "parsed_data = [data for data in parsed_data if data is not None]\n",
    "\n",
    "data = pd.DataFrame(parsed_data)\n",
    "data['cup'] = data['cup'].apply(lambda x: [float(i) for i in x])\n",
    "data['cupImagename'] = data['cupImagename'].str.replace(r'cup_images\\\\', '', regex=True)\n",
    "\n",
    "data.head()\n",
    "data.to_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   cupImagename                         cup\n",
      "0  cup(100).jpg   [156.0, 69.0, 58.0, 95.0]\n",
      "1  cup(101).jpg   [171.0, 9.0, 53.0, 165.0]\n",
      "2  cup(102).jpg    [38.0, 88.0, 21.0, 67.0]\n",
      "3  cup(103).jpg  [148.0, 28.0, 72.0, 187.0]\n",
      "4  cup(104).jpg    [2.0, 51.0, 49.0, 125.0]\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "# Function to convert string representation of list to actual list\n",
    "\n",
    "\n",
    "# Optionally, drop the unnamed column if it's just an index\n",
    "if 'Unnamed: 0' in data.columns:\n",
    "    data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Display the modified dataframe\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as T\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "class CupDataset(Dataset):\n",
    "    def __init__(self, dataframe, root_dir, transforms_2 = None):\n",
    "\n",
    "        self.dataframe = dataframe\n",
    "        self.transforms_2 = transforms_2\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
    "        img = cv2.imread(img_name)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Image not found: {img_name}\")\n",
    "        image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "\n",
    "        # Scale the image pixel values to [0, 255] if necessary\n",
    "        if image.dtype == np.float32:\n",
    "            image = (image * 255).astype(np.uint8)\n",
    "       \n",
    "        # Get dimensions of the image\n",
    "        orig_height, orig_width = image.shape[:2]\n",
    "\n",
    "        image_id = torch.tensor([idx], dtype=torch.int64)\n",
    "        box = torch.tensor(self.dataframe.iloc[idx, 1], dtype=torch.float32)\n",
    "\n",
    "        y_max, y_min, x_max, x_min = box\n",
    "\n",
    "\n",
    "        scale_x = 224.0 / orig_width\n",
    "        scale_y = 224.0 / orig_height   \n",
    "\n",
    "\n",
    "        # Format the box for model input\n",
    "        new_box = torch.tensor([x_min * scale_x, y_min * scale_y, x_max * scale_x, y_max * scale_y], dtype=torch.float32)\n",
    "        labels = torch.ones((1,), dtype=torch.int64)\n",
    "        area = (new_box[3] - new_box[1]) * (new_box[2] - new_box[0])\n",
    "\n",
    "\n",
    "        iscrowd = torch.zeros((new_box.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        image = transform(image)\n",
    "        # image = np.array(image)\n",
    "        # print(image)\n",
    "        \n",
    "\n",
    "        # Create target dictionary\n",
    "        target = {}\n",
    "        target[\"boxes\"] = new_box\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms_2:\n",
    "            sample = self.transforms_2(image = image,\n",
    "                                     bboxes = target['boxes'],\n",
    "                                     labels = labels)\n",
    "            \n",
    "            image = sample['image']\n",
    "            target['boxes'] = torch.Tensor(sample['bboxes'])\n",
    "        # if self.transform is not None:\n",
    "        #     image, target = self.transform(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "def get_object_detection_model(num_classes):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "  \n",
    "    return A.Compose([\n",
    "        ToTensorV2(p=1.0)\n",
    "    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
    "\n",
    "    \n",
    "#cup_dataset = CupDataset(dataframe=data, root_dir='C:/Users/denis/Desktop/HNS/Git/svec-safar-HNS/Zadanie4/cup_images/', transforms_2=get_transform())\n",
    "\n",
    "cup_dataset = CupDataset(dataframe=data, root_dir='C:/Users/denis/Desktop/HNS/Git/svec-safar-HNS/Zadanie4/cup_images/')\n",
    "\n",
    "\n",
    "# Create a data loader\n",
    "\n",
    "indices = np.random.permutation(len(cup_dataset)).tolist()\n",
    "\n",
    "train_data = torch.utils.data.Subset(cup_dataset, indices[:])\n",
    "\n",
    "data_loader = DataLoader(train_data, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boxes': tensor([ 85., 110.,  36.,   1.]), 'labels': tensor([1]), 'image_id': tensor([29]), 'area': tensor(5341.), 'iscrowd': tensor([0, 0, 0, 0])}\n"
     ]
    }
   ],
   "source": [
    "for images, targets in data_loader:\n",
    "    print(targets[0])\n",
    "    break\n",
    "\n",
    "n_batches = len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\denis\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\denis\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "model = get_object_detection_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "metric = MeanAveragePrecision()\n",
    "\n",
    "# Define the lists to store the loss values for training and validation\n",
    "train_loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "def train_model(model, data_loader=None, num_epoch=10):\n",
    "\n",
    "    for epoch in range(1, num_epoch + 1):\n",
    "        print(f\"Starting epoch {epoch} of {num_epoch}\")\n",
    "\n",
    "        time_start = time.time()\n",
    "        loss_accum = 0.0\n",
    "        # loss_mask_accum = 0.0\n",
    "        loss_classifier_accum = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        \n",
    "        for batch_idx, (images, targets) in enumerate(data_loader, 1):\n",
    "\n",
    "            # Predict\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Logging\n",
    "            # loss_mask = loss_dict['loss_mask'].item()\n",
    "            loss_accum += loss.item()\n",
    "            # loss_mask_accum += loss_mask\n",
    "            loss_classifier_accum += loss_dict['loss_classifier'].item()\n",
    "\n",
    "            if batch_idx % 500 == 0:\n",
    "                print(f\"    [Batch {batch_idx:3d} / {n_batches:3d}] Batch train loss: {loss.item():7.3f}.\")\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # Train losses\n",
    "        train_loss = loss_accum / n_batches\n",
    "        \n",
    "        # Store the loss value for training\n",
    "        train_loss_values.append(train_loss)\n",
    "        \n",
    "        # train_loss_mask = loss_mask_accum / n_batches\n",
    "        train_loss_classifier = loss_classifier_accum / n_batches\n",
    "\n",
    "        elapsed = time.time() - time_start\n",
    "\n",
    "        torch.save(model.state_dict(), f\"pytorch_model-e{epoch}.bin\")\n",
    "        prefix = f\"[Epoch {epoch:2d} / {num_epoch:2d}]\"\n",
    "        # print(prefix)\n",
    "        # print(f\"{prefix} Train mask-only loss: {train_loss_mask:7.3f}, classifier loss {train_loss_classifier:7.3f}\")\n",
    "        print(f\"{prefix} Train loss: {train_loss:7.3f} [{elapsed:.0f} secs]\", end=' | ')\n",
    "        \n",
    "        preds_single = []\n",
    "        targets_single = []\n",
    "        \n",
    "    \n",
    "        \n",
    "        metric.update(preds_single, targets_single)\n",
    "        batch_map = metric.compute()\n",
    "        val_loss_values.append(batch_map)\n",
    "        print(f\"Val mAP: {batch_map['map']}\")\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 of 25\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Expected target boxes to be a tensor of shape [N, 4], got torch.Size([4]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4480\\3913182485.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m25\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4480\\3222459976.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, data_loader, num_epoch)\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[0mloss_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1525\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1529\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     65\u001b[0m                     \u001b[0mboxes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"boxes\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                         torch._assert(\n\u001b[0m\u001b[0;32m     68\u001b[0m                             \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mboxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                             \u001b[1;34mf\"Expected target boxes to be a tensor of shape [N, 4], got {boxes.shape}.\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\denis\\anaconda3\\lib\\site-packages\\torch\\__init__.py\u001b[0m in \u001b[0;36m_assert\u001b[1;34m(condition, message)\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1403\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_assert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1404\u001b[1;33m     \u001b[1;32massert\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[1;31m################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Expected target boxes to be a tensor of shape [N, 4], got torch.Size([4])."
     ]
    }
   ],
   "source": [
    "num_epoch = 25\n",
    "model = train_model(model, data_loader, num_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
